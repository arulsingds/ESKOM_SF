{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "505f024f-c8bb-4390-a415-e18504225d76",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2605\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from datetime import datetime\n",
    "from pyspark.dbutils import DBUtils\n",
    "import re\n",
    "\n",
    "import pyodbc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType \n",
    "from pyspark.sql.functions import from_json, col, explode,collect_list\n",
    "\n",
    "# Initialize Spark session (Databricks environment should have this pre-configured)\n",
    "spark = SparkSession.builder.appName(\"Energy Consumption Prediction\").getOrCreate()\n",
    "\n",
    "dbutils = DBUtils(spark)\n",
    "# Retrieve parameter passed to the notebook\n",
    "# Get Forecasters input from Databricks task id\n",
    "databrick_task_id = dbutils.widgets.get(\"DatabrickTaskID\")\n",
    "\n",
    "print(databrick_task_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a1b67db-0d13-4aad-8df6-b53d3828a6d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Read data from SQL Server\n",
    "server_name = \"jdbc:sqlserver://esk-maz-sdb-san-dev-01.database.windows.net\"\n",
    "database_name = \"ESK-MAZ-SDB-SAN-DEV-01\"\n",
    "url = server_name + \";\" + \"databaseName=\" + database_name + \";\"\n",
    "table_DBT = \"dbo.DataBrickTasks\"\n",
    "table_UFM = \"dbo.UserForecastMethod\"\n",
    "\n",
    "table_actual = \"dbo.ActualData\" \n",
    "table_version = \"dbo.DimVersion\"\n",
    "table_forecast = \"dw.ForecastActive\"\n",
    "user = \"arul\"\n",
    "password = \"aari@Singds.8734\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d0b68d8-d167-4a3c-9c6f-c56fc5a9bc5e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nSELECT TOP 1\n    ufm.StartDate,\n    ufm.EndDate,\n    ufm.Parameters,\n    ufm.Customer,\n    ufm.Region,\n    ufm.Status,\n    ufm.ForecastMethodID,\n    ufm.UserForecastMethodID,\n    ufm.JSONCustomer as CustomerJSON,\n    ufm.varJSON,  \n    dfm.Method,\n    dbt.DatabrickID\nFROM \n    [dbo].[DataBrickTasks] AS dbt\nINNER JOIN \n    [dbo].[UserForecastMethod] AS ufm ON dbt.UserForecastMethodID = ufm.UserForecastMethodID\nINNER JOIN \n    [dbo].[DimForecastMethod] AS dfm ON ufm.ForecastMethodID = dfm.ForecastMethodID\n\nWHERE  dbt.DatabrickID=2605 and ufm.ForecastMethodID = 2 and dbt.ExecutionStatus='Failed' \n\nORDER BY\n    dbt.CreationDate\n\n\n"
     ]
    }
   ],
   "source": [
    "# Get Forecasters input from Databricks task id\n",
    "# Define a SQL query to retrieve various forecasting details associated with a specific Databricks task.\n",
    "# This includes forecast methods, customer details, regional data, and task execution status.\n",
    "\n",
    "\n",
    "# ufm.Name\n",
    "query = f\"\"\"\n",
    "SELECT TOP 1\n",
    "    ufm.StartDate,\n",
    "    ufm.EndDate,\n",
    "    ufm.Parameters,\n",
    "    ufm.Region,\n",
    "    ufm.Status,\n",
    "    ufm.ForecastMethodID,\n",
    "    ufm.UserForecastMethodID,\n",
    "    ufm.JSONCustomer as CustomerJSON,\n",
    "    ufm.varJSON,  \n",
    "    dfm.Method,\n",
    "    dbt.DatabrickID\n",
    "FROM \n",
    "    [dbo].[DataBrickTasks] AS dbt\n",
    "INNER JOIN \n",
    "    [dbo].[UserForecastMethod] AS ufm ON dbt.UserForecastMethodID = ufm.UserForecastMethodID\n",
    "INNER JOIN \n",
    "    [dbo].[DimForecastMethod] AS dfm ON ufm.ForecastMethodID = dfm.ForecastMethodID\n",
    "\n",
    "WHERE  dbt.DatabrickID={databrick_task_id} and ufm.ForecastMethodID = 2 and dbt.ExecutionStatus='In Progress' \n",
    "\n",
    "ORDER BY\n",
    "    dbt.CreationDate\n",
    "\n",
    "\"\"\"\n",
    "print(query)\n",
    "\n",
    "# WHERE ExecutionStatus IN ('In Progress')\n",
    "\n",
    "# Read data using Spark SQL by setting up the database connection and executing the SQL query.\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"query\", query) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .load()\n",
    "\n",
    "# Assuming you need to convert the Spark DataFrame to a Pandas DataFrame\n",
    "# If the resulting DataFrame from your query matches what you want to convert to Pandas, you can do so directly\n",
    "# Extract specific fields from the DataFrame, convert them to a Pandas DataFrame, and store in variables.\n",
    "\n",
    "Forecast_Method_Name = df.select(\"Method\").toPandas().iloc[0]['Method']\n",
    "Model_Parmeters=df.select(\"Parameters\").toPandas().iloc[0]['Parameters']\n",
    "UFMID=df.select(\"UserForecastMethodID\").toPandas().iloc[0]['UserForecastMethodID']\n",
    "# CustomerID=df.select(\"Customer\").toPandas().iloc[0]['Customer']\n",
    "\n",
    "StartDate=df.select(\"StartDate\").toPandas().iloc[0]['StartDate']\n",
    "EndDate=df.select(\"EndDate\").toPandas().iloc[0]['EndDate']\n",
    "DatabrickID=df.select(\"DatabrickID\").toPandas().iloc[0]['DatabrickID']\n",
    "Hyper_Parameters=df.select(\"Parameters\").toPandas().iloc[0]['Parameters']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05881e88-c609-4236-af22-698dbecf7dc5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[StartDate: date, EndDate: date, Parameters: string, Customer: string, Region: string, Status: string, ForecastMethodID: int, UserForecastMethodID: int, CustomerJSON: string, varJSON: string, Method: string, DatabrickID: int]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ece54975-097c-4be0-95f3-c73212864032",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SARIMA 693 2605 1 (1,1,1)\n"
     ]
    }
   ],
   "source": [
    "print(Forecast_Method_Name,UFMID,DatabrickID,Hyper_Parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b010e5a-0550-4f7b-8ac5-95e065709bf5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "693\n"
     ]
    }
   ],
   "source": [
    "print(UFMID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf98e381-3714-4463-b2fc-f107e3ea7fa0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comma-separated Customer IDs:\n49,109,121,169,229,241,289,300,567\n"
     ]
    }
   ],
   "source": [
    "\n",
    "json_schema = ArrayType(StructType([\n",
    "    StructField(\"CustomerID\", StringType(), True)\n",
    "]))\n",
    "\n",
    "\n",
    "# df_cust.show()\n",
    "\n",
    "if \"CustomerJSON\" in df.columns:\n",
    "    json_schema = ArrayType(StructType([\n",
    "        StructField(\"CustomerID\", StringType(), True)\n",
    "    ]))\n",
    "\n",
    "    # Parse the JSON string into a column of arrays of structs\n",
    "    df_cust = df.withColumn(\"ParsedJSON\", from_json(\"CustomerJSON\", json_schema))\\\n",
    "                .select(explode(\"ParsedJSON\").alias(\"CustomerDetails\"))\\\n",
    "                .select(col(\"CustomerDetails.CustomerID\"))\n",
    "\n",
    "\n",
    "\n",
    "    # Collect the IDs into a list\n",
    "    multiple_customer_ids_list = df_cust.agg(collect_list(\"CustomerID\")).first()[0]\n",
    "\n",
    "    # Convert the list to a comma-separated string\n",
    "\n",
    "\n",
    " # Convert the list to a comma-separated string\n",
    "    if multiple_customer_ids_list:\n",
    "        multiple_customer_ids_list = ','.join(multiple_customer_ids_list)\n",
    "    else:\n",
    "        multiple_customer_ids_list = ''\n",
    "\n",
    "    # Output the comma-separated IDs\n",
    "    print(\"Comma-separated Customer IDs:\")\n",
    "    print(multiple_customer_ids_list)\n",
    "\n",
    "else:\n",
    "    print(\"Column 'CustomerJSON' does not exist in the dataframe.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c44d2ab7-1900-4417-94cc-814598246618",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['StartDate', 'EndDate', 'Parameters', 'Customer', 'Region', 'Status', 'ForecastMethodID', 'UserForecastMethodID', 'CustomerJSON', 'varJSON', 'Method', 'DatabrickID']\nTrue\nPeak,Standard\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n",
    "if \"varJSON\" in df.columns:\n",
    "    print(\"True\")\n",
    "    VarJsonSchema=ArrayType(\n",
    "                        StructType([\n",
    "                                    StructField(\"VariableID\",StringType(),True)\n",
    "                                ])\n",
    "\n",
    "                            )\n",
    "    df_var=df.withColumn(\"ParsedVarJson\",from_json(\"varJSON\",VarJsonSchema))\\\n",
    "         .select(explode(\"ParsedVarJson\").alias(\"SelectedVarList\"))\\\n",
    "         .select(col(\"SelectedVarList.VariableID\")).alias(\"VariableID\")\n",
    "    all_variables=df_var.agg(collect_list(\"VariableID\")).first()[0] \n",
    "    all_variables=','.join(all_variables)\n",
    "\n",
    "print(all_variables)\n",
    "\n",
    "# Ensure all_variables is a list\n",
    "if isinstance(all_variables, str):\n",
    "    all_variables = all_variables.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30f863f0-c474-4598-af75-35428cd26a98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Standard', 'Peak'}\n['ReportingMonth', 'CustomerID', 'StandardConsumption', 'PeakConsumption']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create DataFrame\n",
    "\n",
    "\n",
    "# Sample AllVariables DataFrame\n",
    "# all_variables = [\"Peak\"]\n",
    "\n",
    "# Define the required columns mapping\n",
    "columns_mapping = {\n",
    "    frozenset([\"PeakConsumption\", \"StandardConsumption\", \"OffPeakConsumption\"]): [\"ReportingMonth\", \"CustomerID\", \"OffpeakConsumption\", \"StandardConsumption\", \"PeakConsumption\"],\n",
    "    frozenset([\"PeakConsumption\", \"StandardConsumption\"]): [\"ReportingMonth\", \"CustomerID\", \"StandardConsumption\", \"PeakConsumption\"],\n",
    "    frozenset([\"PeakConsumption\", \"OffPeakConsumption\"]): [\"ReportingMonth\", \"CustomerID\", \"OffpeakConsumption\", \"PeakConsumption\"],\n",
    "    frozenset([\"StandardConsumption\", \"OffPeakConsumption\"]): [\"ReportingMonth\", \"CustomerID\", \"OffpeakConsumption\", \"StandardConsumption\"],\n",
    "    frozenset([\"PeakConsumption\"]): [\"ReportingMonth\", \"CustomerID\", \"PeakConsumption\"],\n",
    "    frozenset([\"StandardConsumption\"]): [\"ReportingMonth\", \"CustomerID\", \"StandardConsumption\"],\n",
    "    frozenset([\"OffPeakConsumption\"]): [\"ReportingMonth\", \"CustomerID\", \"OffpeakConsumption\"]\n",
    "}\n",
    "\n",
    "# Convert AllVariables to a set for easy comparison\n",
    "all_variables_set = set(all_variables)\n",
    "print(all_variables_set)\n",
    "\n",
    "# Find the matching key in the columns_mapping\n",
    "matching_key = None\n",
    "for key in columns_mapping.keys():\n",
    "    # print(key)\n",
    "    if key.issubset(all_variables_set):\n",
    "        matching_key = key\n",
    "        break\n",
    "\n",
    "# Select the appropriate columns based on the matching key\n",
    "if matching_key:\n",
    "    selected_columns = columns_mapping[matching_key]\n",
    "\n",
    "    print(selected_columns)\n",
    "else:\n",
    "    print(\"No matching columns found in AllVariables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ded03b6-f0d8-409e-b6d8-7e3dc0aa020a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['StandardConsumption', 'PeakConsumption']\n"
     ]
    }
   ],
   "source": [
    "print(selected_columns[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52458990-3e0c-43da-b25a-7d4edbad68c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4659ecc-0247-4c24-8501-4d9c2a69f028",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(SELECT * FROM dbo.ActualData WHERE CustomerID  IN (49,109,121,169,229,241,289,300,567)) AS subquery\n"
     ]
    }
   ],
   "source": [
    "# Construct a SQL query to select all records from the actuals table for a specific customer.\n",
    "query_act_cons = f\"(SELECT * FROM {table_actual} WHERE CustomerID  IN ({multiple_customer_ids_list})) AS subquery\"\n",
    "\n",
    "\n",
    "# Read data from a JDBC source using the Spark SQL DataFrameReader. This operation involves connecting to the database\n",
    "# and executing the query to fetch actual consumption data for the specified customer.\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"dbtable\", query_act_cons) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .load()\n",
    "\n",
    "# Convert the Spark DataFrame to a Pandas DataFrame to use with time series analysis in Python.\n",
    "pandas_df = df.select(*selected_columns).toPandas()\n",
    "pandas_df['CustomerID'] = pandas_df['CustomerID'].astype(str)\n",
    "\n",
    "#pandas_df['ReportingMonth'] = pd.to_datetime(pandas_df['ReportingMonth'])\n",
    "pandas_df['ReportingMonth'] = pd.to_datetime(pandas_df['ReportingMonth']).dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "print(query_act_cons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "213f4961-1886-4a49-b70b-7cc4fa0c856a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2024-08-01', '2024-09-01', '2024-10-01', '2024-11-01',\n               '2024-12-01', '2025-01-01', '2025-02-01', '2025-03-01',\n               '2025-04-01', '2025-05-01', '2025-06-01', '2025-07-01',\n               '2025-08-01', '2025-09-01', '2025-10-01', '2025-11-01',\n               '2025-12-01', '2026-01-01', '2026-02-01', '2026-03-01',\n               '2026-04-01', '2026-05-01', '2026-06-01', '2026-07-01'],\n              dtype='datetime64[ns]', freq='MS') Actual_last_date 2024-03-01 00:00:00 No of month to predict for 24\n"
     ]
    }
   ],
   "source": [
    "# Find the most recent reporting month in the data, which will be used to determine the starting point for forecasting.\n",
    "Actuals_last_date = pandas_df['ReportingMonth'].max() \n",
    "\n",
    "\n",
    "# Generate a date range from the start date to the end date with a monthly frequency, starting on the first of each month.\n",
    "# This range represents the forecast period.\n",
    "\n",
    "forecast_dates = pd.date_range(start=StartDate, end=EndDate, freq='MS')[0:]\n",
    "print(forecast_dates,f\"Actual_last_date {Actuals_last_date}\",f\"No of month to predict for {len(forecast_dates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8347ec52-72e3-4eda-9062-cd9a88f63430",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of periods to forecast: 24\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate n_periods as the number of months between the last historical date\n",
    "# and the last date you want to forecast\n",
    "n_periods = len(forecast_dates)\n",
    "\n",
    "print(f\"Number of periods to forecast: {n_periods}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "722d41cf-aecd-4b86-bd3b-56f145c4a474",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['109' '121' '169' '229' '241' '289' '49']\n"
     ]
    }
   ],
   "source": [
    "# validate if data is only processed for intended customer\n",
    "\n",
    "unique_customers = pandas_df['CustomerID'].unique()\n",
    "print(unique_customers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9e0f4a5-5a2a-49a8-9400-fbeca77adb7a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3474730922484391>, line 10\u001B[0m\n",
       "\u001B[1;32m      3\u001B[0m parameter_set2 \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m)  \u001B[38;5;66;03m# Default value for SARIMA seasonal_order\u001B[39;00m\n",
       "\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m Forecast_Method_Name \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSARIMA\u001B[39m\u001B[38;5;124m'\u001B[39m:\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Extract parameters for SARIMA\u001B[39;00m\n",
       "\u001B[1;32m      7\u001B[0m \n",
       "\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# This regular expression finds tuples within parentheses.\u001B[39;00m\n",
       "\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Splitting the string to isolate order and seasonal_order\u001B[39;00m\n",
       "\u001B[0;32m---> 10\u001B[0m     order, seasonal_order\u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39mfindall(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m(.*?\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m'\u001B[39m,Hyper_Parameters)   \u001B[38;5;66;03m#['(1,1,1),(1,1,1,2)']\u001B[39;00m\n",
       "\u001B[1;32m     11\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(Order_Paramaters \u001B[39m\u001B[38;5;132;01m{\u001B[39;00morder\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m  Seasonal_Paramaters \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mseasonal_order\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)  \n",
       "\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Remove parentheses and split the parameters by commas to extract individual elements.\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mValueError\u001B[0m: not enough values to unpack (expected 2, got 1)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ValueError",
        "evalue": "not enough values to unpack (expected 2, got 1)"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>ValueError</span>: not enough values to unpack (expected 2, got 1)"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
        "File \u001B[0;32m<command-3474730922484391>, line 10\u001B[0m\n\u001B[1;32m      3\u001B[0m parameter_set2 \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m)  \u001B[38;5;66;03m# Default value for SARIMA seasonal_order\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m Forecast_Method_Name \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSARIMA\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Extract parameters for SARIMA\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# This regular expression finds tuples within parentheses.\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Splitting the string to isolate order and seasonal_order\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m     order, seasonal_order\u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39mfindall(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m(.*?\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m'\u001B[39m,Hyper_Parameters)   \u001B[38;5;66;03m#['(1,1,1),(1,1,1,2)']\u001B[39;00m\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(Order_Paramaters \u001B[39m\u001B[38;5;132;01m{\u001B[39;00morder\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m  Seasonal_Paramaters \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mseasonal_order\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)  \n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Remove parentheses and split the parameters by commas to extract individual elements.\u001B[39;00m\n",
        "\u001B[0;31mValueError\u001B[0m: not enough values to unpack (expected 2, got 1)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize default parameter sets\n",
    "parameter_set1 = (0, 0, 0)  # Default value for ARIMA order\n",
    "parameter_set2 = (0, 0, 0, 0)  # Default value for SARIMA seasonal_order\n",
    "\n",
    "if Forecast_Method_Name == 'SARIMA':\n",
    "# Extract parameters for SARIMA\n",
    "\n",
    "# This regular expression finds tuples within parentheses.\n",
    "# Splitting the string to isolate order and seasonal_order\n",
    "    order, seasonal_order= re.findall(r'\\(.*?\\)',Hyper_Parameters)   #['(1,1,1),(1,1,1,2)']\n",
    "    print(f\"(Order_Paramaters {order}  Seasonal_Paramaters {seasonal_order}\")  \n",
    "\n",
    "# Remove parentheses and split the parameters by commas to extract individual elements.\n",
    "order = order.strip('()')\n",
    "order_parameters = order.split(',')\n",
    "\n",
    "seasonal_order=seasonal_order.strip('()')\n",
    "seasonal_order_parameters = seasonal_order.split(',')\n",
    "\n",
    "\n",
    "\n",
    "# Assign the values to the variables\n",
    "p = int(order_parameters[0])\n",
    "d = int(order_parameters[1])\n",
    "q = int(order_parameters[2])\n",
    "\n",
    "\n",
    "# Convert extracted string parameters into integers and assign them to corresponding variables. \n",
    " \n",
    "s_p = int(seasonal_order_parameters[0])\n",
    "s_d = int(seasonal_order_parameters[1])\n",
    "s_q = int(seasonal_order_parameters[2])\n",
    "s_m = int(seasonal_order_parameters[3])\n",
    "\n",
    "print(f\"(Order_Paramaters {p,d,q}  Seasonal_Paramaters {s_p,s_d,s_q,s_m}\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9e971c5-0f1e-43fa-a25b-f0e8edfe49f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3474730922484391>, line 10\u001B[0m\n",
       "\u001B[1;32m      3\u001B[0m parameter_set2 \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m)  \u001B[38;5;66;03m# Default value for SARIMA seasonal_order\u001B[39;00m\n",
       "\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m Forecast_Method_Name \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSARIMA\u001B[39m\u001B[38;5;124m'\u001B[39m:\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Extract parameters for SARIMA\u001B[39;00m\n",
       "\u001B[1;32m      7\u001B[0m \n",
       "\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# This regular expression finds tuples within parentheses.\u001B[39;00m\n",
       "\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Splitting the string to isolate order and seasonal_order\u001B[39;00m\n",
       "\u001B[0;32m---> 10\u001B[0m     order, seasonal_order\u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39mfindall(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m(.*?\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m'\u001B[39m,Hyper_Parameters)   \u001B[38;5;66;03m#['(1,1,1),(1,1,1,2)']\u001B[39;00m\n",
       "\u001B[1;32m     11\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(Order_Paramaters \u001B[39m\u001B[38;5;132;01m{\u001B[39;00morder\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m  Seasonal_Paramaters \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mseasonal_order\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)  \n",
       "\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Remove parentheses and split the parameters by commas to extract individual elements.\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mValueError\u001B[0m: not enough values to unpack (expected 2, got 1)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {
        "DatabrickTaskID": "2605"
       },
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ValueError",
        "evalue": "not enough values to unpack (expected 2, got 1)"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
        "File \u001B[0;32m<command-3474730922484391>, line 10\u001B[0m\n\u001B[1;32m      3\u001B[0m parameter_set2 \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m)  \u001B[38;5;66;03m# Default value for SARIMA seasonal_order\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m Forecast_Method_Name \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSARIMA\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Extract parameters for SARIMA\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# This regular expression finds tuples within parentheses.\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Splitting the string to isolate order and seasonal_order\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m     order, seasonal_order\u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39mfindall(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m(.*?\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m'\u001B[39m,Hyper_Parameters)   \u001B[38;5;66;03m#['(1,1,1),(1,1,1,2)']\u001B[39;00m\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(Order_Paramaters \u001B[39m\u001B[38;5;132;01m{\u001B[39;00morder\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m  Seasonal_Paramaters \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mseasonal_order\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)  \n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Remove parentheses and split the parameters by commas to extract individual elements.\u001B[39;00m\n",
        "\u001B[0;31mValueError\u001B[0m: not enough values to unpack (expected 2, got 1)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(Forecast_Method_Name, n_periods,len(forecast_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cb016ac-75d6-455b-a0e2-1917283cd39d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n  self._init_dates(dates, freq)\n/databricks/python/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n  self._init_dates(dates, freq)\n/databricks/python/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n  self._init_dates(dates, freq)\n/databricks/python/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n  self._init_dates(dates, freq)\n/databricks/python/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n  self._init_dates(dates, freq)\n/databricks/python/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n  self._init_dates(dates, freq)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast done with SARIMA model\nForecast done with SARIMA model\nForecast done with SARIMA model\nForecast done with SARIMA model\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n  self._init_dates(dates, freq)\n/databricks/python/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n  self._init_dates(dates, freq)\n/databricks/python/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n  self._init_dates(dates, freq)\n/databricks/python/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n  self._init_dates(dates, freq)\n/databricks/python/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n  self._init_dates(dates, freq)\n/databricks/python/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n  self._init_dates(dates, freq)\n/databricks/python/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n  self._init_dates(dates, freq)\n/databricks/python/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n  self._init_dates(dates, freq)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast done with SARIMA model\nForecast done with SARIMA model\nForecast done with SARIMA model\nForecast done with SARIMA model\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n  self._init_dates(dates, freq)\n/databricks/python/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n  self._init_dates(dates, freq)\n/databricks/python/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n  self._init_dates(dates, freq)\n/databricks/python/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n  self._init_dates(dates, freq)\n/databricks/python/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n  self._init_dates(dates, freq)\n/databricks/python/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n  self._init_dates(dates, freq)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast done with SARIMA model\nForecast done with SARIMA model\nForecast done with SARIMA model\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n  self._init_dates(dates, freq)\n/databricks/python/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n  self._init_dates(dates, freq)\n/databricks/python/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n  self._init_dates(dates, freq)\n/databricks/python/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n  self._init_dates(dates, freq)\n/databricks/python/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n  self._init_dates(dates, freq)\n/databricks/python/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n  self._init_dates(dates, freq)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast done with SARIMA model\nForecast done with SARIMA model\nForecast done with SARIMA model\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n  self._init_dates(dates, freq)\n/databricks/python/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n  self._init_dates(dates, freq)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def automated_forecasts_for_all_types(data,  selected_columns,n_periods=n_periods,):\n",
    "    if selected_columns is None:\n",
    "        selected_columns = [\"OffpeakConsumption\", \"StandardConsumption\", \"PeakConsumption\"]\n",
    "\n",
    "    all_forecasts = []\n",
    "\n",
    "    # Determine which consumption types are in the selected columns\n",
    "    consumption_types = [\"OffpeakConsumption\", \"StandardConsumption\", \"PeakConsumption\"]\n",
    "    cons_types = [col for col in selected_columns if col in consumption_types]\n",
    "    # print(cons_types)\n",
    "\n",
    "    # Iterate through each unique customer ID in the dataset.\n",
    "    for customer_id in data['CustomerID'].unique():\n",
    "        customer_forecasts = {}\n",
    "\n",
    "        # Ensure data is sorted by ReportingMonth\n",
    "        customer_data = data[data['CustomerID'] == customer_id].sort_values('ReportingMonth')\n",
    "\n",
    "        # Loop through each type of consumption to forecast individually.        \n",
    "        for cons_type in cons_types:\n",
    "            # Prepare the time series  by setting ReportingMonth as the index\n",
    "            series = customer_data.set_index('ReportingMonth')[cons_type]\n",
    "\n",
    "            forecast=None\n",
    "\n",
    "            if Forecast_Method_Name == 'SARIMA':\n",
    "                # Fit SARIMA model\n",
    "                model = SARIMAX(series, order=(1,2,1), seasonal_order=(1,2,1,3))               \n",
    "                #model = SARIMAX(series, order=(q,d,p), seasonal_order=(s_p,s_d,s_q,s_m))\n",
    "                model_fit = model.fit(disp=False)\n",
    "                forecast = model_fit.forecast(steps=n_periods)\n",
    "                print(\"Forecast done with SARIMA model\")\n",
    "\n",
    "            # if forecast is not None:\n",
    "            #  forecast[forecast < 0] = forecast*-1\n",
    "\n",
    "\n",
    "            customer_forecasts[cons_type] = forecast\n",
    "               # Construct a DataFrame to store the forecast results\n",
    "        forecast_df_data = {\n",
    "                                'ReportingMonth': forecast_dates,\n",
    "                                'CustomerID': [customer_id] * n_periods\n",
    "                            }\n",
    "        for cons_type in cons_types:\n",
    "            forecast_df_data[cons_type] = customer_forecasts.get(cons_type, [None] * n_periods)\n",
    "\n",
    "\n",
    "        forecast_df = pd.DataFrame(forecast_df_data)        \n",
    "        # Append each customer's forecast DataFrame to a list.\n",
    "        all_forecasts.append(forecast_df)\n",
    "\n",
    "    # Concatenate all individual forecast DataFrames into one.\n",
    "    forecast_combined_df = pd.concat(all_forecasts, ignore_index=True)\n",
    "\n",
    "\n",
    "    # Round the consumption columns if they are present\n",
    "    for cons_type in cons_types:\n",
    "        forecast_combined_df[cons_type] = forecast_combined_df[cons_type].round(2)\n",
    "        # print(forecast_combined_df)\n",
    "\n",
    "    # Return the combined forecast DataFrame.\n",
    "    return forecast_combined_df\n",
    "\n",
    "# Execute the forecasting function with the loaded data.\n",
    "forecast_combined_df = automated_forecasts_for_all_types(pandas_df,selected_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c73d41c2-6a40-436e-abd8-6c41efd383b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ReportingMonth CustomerID  StandardConsumption  PeakConsumption\n0       2024-08-01        109           3816157.73        777960.18\n1       2024-09-01        109           4651832.54        812294.12\n2       2024-10-01        109           5394481.60       -279202.47\n3       2024-11-01        109           6421466.99        130907.30\n4       2024-12-01        109           7673058.53         88173.13\n..             ...        ...                  ...              ...\n163     2026-03-01         49          50098868.35      -9789785.76\n164     2026-04-01         49          55305727.22     -12610796.94\n165     2026-05-01         49          60233932.89     -12485636.46\n166     2026-06-01         49          65501238.78     -13357045.09\n167     2026-07-01         49          71694190.73     -16522733.12\n\n[168 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "print(forecast_combined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91a5910d-0f7d-43d3-8136-ad934e673b24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# if (forecast_combined_df[selected_columns[2:]] < 0).any().any():\n",
    "#         print(\"Negative values detected after transformation:\")\n",
    "#         print(forecast_combined_df[\n",
    "#                                (forecast_combined_df['StandardConsumption'] < 0) |\n",
    "#                                (forecast_combined_df['PeakConsumption'] < 0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e026183-f97b-4db2-962c-e281e1f296af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ReportingMonth CustomerID  OffpeakConsumption  StandardConsumption\n0       2024-08-01        121        7.403130e+06         3.816158e+06\n1       2024-09-01        121        6.414111e+06         4.651833e+06\n2       2024-10-01        121        7.534321e+06         5.394482e+06\n3       2024-11-01        121        9.307516e+06         6.421467e+06\n4       2024-12-01        121        8.437162e+06         7.673059e+06\n..             ...        ...                 ...                  ...\n103     2027-03-01        241        9.880224e+07         1.292894e+08\n104     2027-04-01        241        1.069110e+08         1.388569e+08\n105     2027-05-01        241        1.161396e+08         1.477746e+08\n106     2027-06-01        241        1.199529e+08         1.571062e+08\n107     2027-07-01        241        1.290696e+08         1.679374e+08\n\n[108 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(forecast_combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f91eb59-ed4c-4653-9a06-d32b673322c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "forecast_combined_df['UserForecastMethodID'] = UFMID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ce59ce4-668b-4a2a-b956-16eaa1a3d159",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ReportingMonth CustomerID  ...  StandardConsumption  UserForecastMethodID\n0       2024-08-01        121  ...         3.816158e+06                   639\n1       2024-09-01        121  ...         4.651833e+06                   639\n2       2024-10-01        121  ...         5.394482e+06                   639\n3       2024-11-01        121  ...         6.421467e+06                   639\n4       2024-12-01        121  ...         7.673059e+06                   639\n..             ...        ...  ...                  ...                   ...\n103     2027-03-01        241  ...         1.292894e+08                   639\n104     2027-04-01        241  ...         1.388569e+08                   639\n105     2027-05-01        241  ...         1.477746e+08                   639\n106     2027-06-01        241  ...         1.571062e+08                   639\n107     2027-07-01        241  ...         1.679374e+08                   639\n\n[108 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(forecast_combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44a33d00-5fc6-400d-84ca-b580c893ff79",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       OffpeakConsumption  StandardConsumption  UserForecastMethodID\ncount        1.080000e+02         1.080000e+02                 108.0\nmean         4.690210e+07         5.838803e+07                 639.0\nstd          3.680347e+07         4.917561e+07                   0.0\nmin          6.414111e+06         3.816158e+06                 639.0\n25%          1.552667e+07         1.573354e+07                 639.0\n50%          3.608522e+07         4.362205e+07                 639.0\n75%          7.243804e+07         9.263950e+07                 639.0\nmax          1.290696e+08         1.679374e+08                 639.0\n"
     ]
    }
   ],
   "source": [
    "print(forecast_combined_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b6168ed-03ee-4dcf-bed0-49b146bdfb9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "forecast_combined_spark_df = spark.createDataFrame(forecast_combined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3d60ef4-113a-4c44-a55f-a03c89906d7f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the properties for the database connection\n",
    "write_url = \"jdbc:sqlserver://esk-maz-sdb-san-dev-01.database.windows.net;databaseName=ESK-MAZ-SDB-SAN-DEV-01\"\n",
    "write_properties = {\n",
    "    \"user\": \"arul\",\n",
    "    \"password\": \"aari@Singds.8734\",\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "\n",
    "# Define the name of the target table\n",
    "target_table_name = \"dbo.ForecastFact\"\n",
    "\n",
    "# Write the DataFrame to the SQL table\n",
    "forecast_combined_spark_df.write.jdbc(url=write_url, table=target_table_name, mode=\"append\", properties=write_properties)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54bd90d1-9e38-4ae9-9e8e-10c64ee93b48",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "==============================           Projection     ===================================="
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SARIMA",
   "widgets": {
    "DatabrickTaskID": {
     "currentValue": "2605",
     "nuid": "c3438225-50c9-4ef3-b8c6-be93c56d2e16",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "DatabrickTaskID",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
